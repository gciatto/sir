\subsection{Affine transformations on the plane}
	Here we recall the affine transformation equation, mapping each point from a source 2D plane to another translated, scaled and rotated 2D plane: 
	\begin{equation}
		\left(\begin{array}{c}
			x' \\ y'
		\end{array}\right)
		=
		\left(\begin{array}{cc}
			s_x \cdot \cos{x} & -\sin{x} \\ \sin{x} & s_y \cdot \cos{x}
		\end{array}\right)
		\cdot
		\left(\begin{array}{c}
		x \\ y
		\end{array}\right)
		+
		\left(\begin{array}{c}
		t_x \\ t_y
		\end{array}\right)
	\end{equation}
	and its inverse:
	\begin{equation}
		\left(\begin{array}{c}
			x \\ y
		\end{array}\right)
		=
		\left(\begin{array}{cc}
			\frac{1}{s_x} \cdot \cos{x} & \sin{x} \\ -\sin{x} & \frac{1}{s_y} \cdot \cos{x}
		\end{array}\right)
		\cdot
		\left(\begin{array}{c}
			x' \\ y'
		\end{array}\right)
		-
		\left(\begin{array}{c}
			t_x \\ t_y
		\end{array}\right)
	\end{equation}
	where $(x,\,y)^\top$ is a point in the source reference frame, $(x',\,y')^\top$ is the transformed reference frame, $(t_x,\,t_y)^\top$ is a translation vector, $s_x$ and $s_y$ are scale factors for the horizontal and vertical coordinates, respectively, and $\theta$ is the angle between the source reference frame abscissas axis and the transformed frame one. 

\subsection{Multivariate normal distribution}
	Here we recall the notion of multivariate normal distribution and its linearity and geometric properties.
	Let $\vect{x} \in \mathbb{R}^n$ be a normally distributed vector having mean $\vect{\mu} \in \mathbb{R}^n$ and covariances matrix $\Sigma \in \mathcal{M}_{n \times n}(\mathbb{R})$, then we write: 
	\[
		\vect{x} \sim \mathcal{N}(\vect{\mu},\, \Sigma)
	\]
	meaning that the probability density function of $\vect{x}$ is the multidimensional Gaussian function:
	\[
		pdf(\vect{x}) = \frac{1}{\sqrt{(2\pi)^n \cdot |\Sigma|}} \cdot \mathrm{e}^{-\frac{1}{2} \cdot (\vect{x} - \vect{\mu})^\top \cdot \Sigma^{-1} \cdot (\vect{x} - \vect{\mu})}
	\]
	
	\paragraph{Linearity.}
	
		Let $\vect{y} \in \mathbb{R}^m$ be a random vector, obtained by linearly combining a number of normally distributed independent random vectors $\vect{x}_i \sim \mathcal{N}(\vect{\mu}_i,\, \Sigma_i)$:
		\[
			\vect{y} = A_1 \cdot \vect{x}_1 + A_2 \cdot \vect{x}_2 + \ldots + \vect{b}
		\]
		where $A_i \in \mathcal{M}_{m \times n}(\mathbb{R})$ are transformation matrices and $\vect{b} \in \mathbb{R}^m$ is a constant vector, then $\vect{y}$ is normally distributed too, having mean $\vect{\mu}_y$ and covariances matrix $\Sigma_y$, expressed as follows:
		\begin{equation}
			\vect{\mu}_y = \vect{b} + \sum_{i}^{} A_i \cdot \vect{\mu}_i
		\end{equation}
		\begin{equation}
			\Sigma_y = \sum_{i}^{} A_i \cdot \Sigma_i \cdot A_i^\top
		\end{equation}
		
	\paragraph{Representation.}
		
		Usually, a Multivariate normal distribution $\mathcal{N}(\vect{\mu},\, \Sigma)$ is imagined and represented as an hyper-ellipsoid centered in $\vect{\mu}$, whose axes are the (left) singular vectors of $\Sigma$. Such an ellipsoid is scaled (w.r.t. each axis) according to the singular values of $\Sigma$.
		In order to produce a rendering of such an hyper-ellipsoid (which is an ordinary ellipse in the 2D case), it is sufficient to produce a singular-values-decomposition of the covariances matrix:
		\[
			\Sigma \stackrel{svd}{=} V \cdot D \cdot V^\top = 
			\left(\begin{array}{ccc}
				\vect{v}_1 & \cdots & \vect{v}_n
			\end{array}\right)
			\cdot
			\left(\begin{array}{cccc}
				d_1^2 & 0 & \cdots & 0 \\
				0 & d_2^2 & \cdots & 0 \\
				\vdots & \vdots & \ddots & \vdots \\
				0 & 0 & \cdots & d_n^2
			\end{array}\right)
			\cdot
			\left(\begin{array}{c}
				\vect{v}_1^\top \\
				\vdots \\
				\vect{v}_n^\top
			\end{array}\right)
		\]
		where the $i$-th column of $V$, namely $\vect{v}_i$, is the versor identifying the direction of the $i$-th axis of the ellipsoid, and the $i$-th diagonal element of $D$, namely $d_i^2$ can be thought to be the variance of the distribution according to the $i$-th axis, \ie{} $d_i$ can be thought to be its standard deviation.
		
		In the $1$-dimensional case it is common to represent the $k$-standard deviation interval, \ie{} the circular interval centered on the mean and including each point whose distance from the mean is lower than $k$ times the standard deviation. Analogously, the $k$-th ellipsoid centered in $\vect{\mu}$ can be represented for the $n$-dimensional case by applying the following affine transformation to each point on the unary hyper-sphere:
		\[
			k \cdot 
			\left(\begin{array}{ccc}
				\vect{v}_1 & \cdots & \vect{v}_n
			\end{array}\right)
			\cdot
			\left(\begin{array}{cccc}
				d_1 & 0 & \cdots & 0 \\
				0 & d_2 & \cdots & 0 \\
				\vdots & \vdots & \ddots & \vdots \\
				0 & 0 & \cdots & d_n
			\end{array}\right)
			\cdot
			\vect{c} + \vect{\mu}
		\]
		where $\vect{c} \in \{ (x_1,\, \ldots,\,\ x_n)^\top \ |\  x_1^2 + \ldots + x_n^2 = 1 \}$.
		
	\subsection{Differential drive.}
		\label{app.differential_drive}
	
		A differential robot only owns two non-rotating wheels. Its actuators can simply impose a velocity value to each wheel. We will call $v_l$ and $v_r$ the left and right speed value, while the distance between the wheels is $2L$.
		Here we recall how the linear velocity value $v$ in the heading direction and the angular velocity $\omega$, w.r.t. the \emph{instantaneous center of curvature}, can be computed as a function of $v_l$ and $v_r$:
		\begin{equation}
			\left(\begin{array}{c}
				v \\ \omega
			\end{array}\right)
			=
			\left(\begin{array}{c}
				v_{l,r} \\ 0
			\end{array}\right)
			\ \mathrm{if} \ v_l = v_r = v_{l,r}
		\end{equation}
		\begin{equation}
			\left(\begin{array}{c}
				v \\ 
				\omega
			\end{array}\right)
			=
			\left(\begin{array}{c}
				\frac{v_l + v_r}{2} \\ 
				\frac{v_r - v_l}{2L}
			\end{array}\right)
			\ \mathrm{if} \ v_l \neq v_r
		\end{equation}